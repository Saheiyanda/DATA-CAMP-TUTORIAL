{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e372b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          zipcode   mars1  MARS2  NUMDEP\n",
      "agi_stub                                \n",
      "1         1439444  170320  28480   52490\n",
      "2         1439444  104000  37690   64660\n",
      "3         1439444   39160  45390   47330\n",
      "4         1439444   11670  44410   37760\n",
      "5         1439444    7820  67750   60730\n",
      "6         1439444    1210  16340   16300\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create list of columns to use\n",
    "cols = [\"zipcode\", \"agi_stub\", \"mars1\", \"MARS2\", \"NUMDEP\"]\n",
    "\n",
    "# Create dataframe from csv using only selected columns\n",
    "data = pd.read_csv(\"vt_tax_data_2016-Data Engineering.csv\", usecols=cols)\n",
    "\n",
    "# View counts of dependents and tax returns by income level\n",
    "print(data.groupby(\"agi_stub\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9381880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nrows and skiprows to make a dataframe, vt_data_next500, with the next 500 rows.\n",
    "# Set the header argument so that pandas knows there is no header row.\n",
    "# Name the columns in vt_data_next500 by supplying a list of vt_data_first500's columns to the names argument.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1808031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   STATEFIPS STATE  zipcode  agi_stub      N1  mars1  MARS2  MARS4   PREP  \\\n",
      "0         50    VT        0         1  111580  85090  14170  10740  45360   \n",
      "1         50    VT        0         2   82760  51960  18820  11310  35600   \n",
      "2         50    VT        0         3   46270  19540  22650   3620  24140   \n",
      "3         50    VT        0         4   30070   5830  22190    960  16060   \n",
      "4         50    VT        0         5   39530   3900  33800    590  22500   \n",
      "\n",
      "       N2  ...  N10300  A10300  N85530  A85530  N85300  A85300  N11901  \\\n",
      "0  130630  ...   53660   50699       0       0       0       0   10820   \n",
      "1  132950  ...   74340  221146       0       0       0       0   12820   \n",
      "2   91870  ...   44860  266097       0       0       0       0   10810   \n",
      "3   71610  ...   29580  264678       0       0       0       0    7320   \n",
      "4  103710  ...   39170  731963      40      24       0       0   12500   \n",
      "\n",
      "   A11901  N11902  A11902  \n",
      "0    9734   88260  138337  \n",
      "1   20029   68760  151729  \n",
      "2   24499   34600   90583  \n",
      "3   21573   21300   67045  \n",
      "4   67761   23320  103034  \n",
      "\n",
      "[5 rows x 147 columns]\n",
      "   STATEFIPS STATE  zipcode  agi_stub   N1  mars1  MARS2  MARS4  PREP   N2  \\\n",
      "0         50    VT     5356         2  180    120     40      0    90  250   \n",
      "1         50    VT     5356         3   80     50     40      0    40  150   \n",
      "2         50    VT     5356         4   50      0     40      0    40  110   \n",
      "3         50    VT     5356         5   80     20     50      0    60  170   \n",
      "4         50    VT     5356         6    0      0      0      0     0    0   \n",
      "\n",
      "   ...  N10300  A10300  N85530  A85530  N85300  A85300  N11901  A11901  \\\n",
      "0  ...     170     497       0       0       0       0      50      76   \n",
      "1  ...      80     460       0       0       0       0      40     142   \n",
      "2  ...      50     471       0       0       0       0       0       0   \n",
      "3  ...      80    2229       0       0       0       0      30     531   \n",
      "4  ...       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   N11902  A11902  \n",
      "0     130     212  \n",
      "1      50     148  \n",
      "2      30      87  \n",
      "3      30     246  \n",
      "4       0       0  \n",
      "\n",
      "[5 rows x 147 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe of next 500 rows with labeled columns\n",
    "vt_data_first500 = pd.read_csv(\"vt_tax_data_2016-Data Engineering.csv\")\n",
    "vt_data_next500 = pd.read_csv(\"vt_tax_data_2016-Data Engineering.csv\", \n",
    "                       nrows=500, skiprows=500,\n",
    "                        header= None,\n",
    "                       names=list(vt_data_first500))\n",
    "\n",
    "# View the Vermont dataframes to confirm they're different\n",
    "print(vt_data_first500.head())\n",
    "print(vt_data_next500.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d54dd9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATEFIPS     int64\n",
      "STATE        object\n",
      "zipcode       int64\n",
      "agi_stub      int64\n",
      "N1            int64\n",
      "              ...  \n",
      "A85300        int64\n",
      "N11901        int64\n",
      "A11901        int64\n",
      "N11902        int64\n",
      "A11902        int64\n",
      "Length: 147, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load vt_tax_data_2016.csv with no arguments and view the dataframe's dtypes attribute.\n",
    "# Note the data types of zipcode and agi_stub.\n",
    "\n",
    "# Load csv with no additional arguments\n",
    "data = pd.read_csv(\"vt_tax_data_2016-Data Engineering.csv\")\n",
    "\n",
    "# Print the data types\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "765a744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATEFIPS       int64\n",
      "STATE          object\n",
      "zipcode        object\n",
      "agi_stub     category\n",
      "N1              int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create dict specifying data types for agi_stub and zipcode\n",
    "data_types = {'agi_stub':'category',  'zipcode':'str'}\n",
    "\n",
    "# Load csv using dtype to set correct data types\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\", dtype = data_types)\n",
    "\n",
    "# Print data types of resulting frame\n",
    "print(data.dtypes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2607fa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   STATEFIPS STATE  zipcode  agi_stub      N1  mars1  MARS2  MARS4   PREP  \\\n",
      "0         50    VT      NaN         1  111580  85090  14170  10740  45360   \n",
      "1         50    VT      NaN         2   82760  51960  18820  11310  35600   \n",
      "2         50    VT      NaN         3   46270  19540  22650   3620  24140   \n",
      "3         50    VT      NaN         4   30070   5830  22190    960  16060   \n",
      "4         50    VT      NaN         5   39530   3900  33800    590  22500   \n",
      "5         50    VT      NaN         6    9620    600   8150      0   7040   \n",
      "\n",
      "       N2  ...  N10300  A10300  N85530  A85530  N85300  A85300  N11901  \\\n",
      "0  130630  ...   53660   50699       0       0       0       0   10820   \n",
      "1  132950  ...   74340  221146       0       0       0       0   12820   \n",
      "2   91870  ...   44860  266097       0       0       0       0   10810   \n",
      "3   71610  ...   29580  264678       0       0       0       0    7320   \n",
      "4  103710  ...   39170  731963      40      24       0       0   12500   \n",
      "5   26430  ...    9600  894432    3350    4939    4990   20428    3900   \n",
      "\n",
      "   A11901  N11902  A11902  \n",
      "0    9734   88260  138337  \n",
      "1   20029   68760  151729  \n",
      "2   24499   34600   90583  \n",
      "3   21573   21300   67045  \n",
      "4   67761   23320  103034  \n",
      "5   93123    2870   39425  \n",
      "\n",
      "[6 rows x 147 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create dict specifying that 0s in zipcode are NA values\n",
    "null_values = {'zipcode':0}\n",
    "\n",
    "# Load csv using na_values keyword argument\n",
    "data = pd.read_csv(\"vt_tax_data_2016-Data Engineering.csv\", \n",
    "                   na_values = null_values)\n",
    "\n",
    "# View rows with NA ZIP codes\n",
    "print(data[data.zipcode.isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "529be16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   STATEFIPS STATE  zipcode  agi_stub      N1  mars1  MARS2  MARS4   PREP  \\\n",
      "0         50    VT        0         1  111580  85090  14170  10740  45360   \n",
      "1         50    VT        0         2   82760  51960  18820  11310  35600   \n",
      "2         50    VT        0         3   46270  19540  22650   3620  24140   \n",
      "3         50    VT        0         4   30070   5830  22190    960  16060   \n",
      "4         50    VT        0         5   39530   3900  33800    590  22500   \n",
      "\n",
      "       N2  ...  N10300  A10300  N85530  A85530  N85300  A85300  N11901  \\\n",
      "0  130630  ...   53660   50699       0       0       0       0   10820   \n",
      "1  132950  ...   74340  221146       0       0       0       0   12820   \n",
      "2   91870  ...   44860  266097       0       0       0       0   10810   \n",
      "3   71610  ...   29580  264678       0       0       0       0    7320   \n",
      "4  103710  ...   39170  731963      40      24       0       0   12500   \n",
      "\n",
      "   A11901  N11902  A11902  \n",
      "0    9734   88260  138337  \n",
      "1   20029   68760  151729  \n",
      "2   24499   34600   90583  \n",
      "3   21573   21300   67045  \n",
      "4   67761   23320  103034  \n",
      "\n",
      "[5 rows x 147 columns]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  # Import the CSV without any keyword arguments\n",
    "  data = pd.read_csv('vt_tax_data_2016.csv')\n",
    "  \n",
    "  # View first 5 records\n",
    "  print(data.head())\n",
    "  \n",
    "except pd.errors.ParserError:\n",
    "    print(\"Your data contained rows that could not be parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e010ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#   # Import CSV with error_bad_lines set to skip bad records\n",
    "#   data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\", \n",
    "#                      error_bad_lines = False)\n",
    "  \n",
    "#   # View first 5 records\n",
    "#   print(data.head())\n",
    "  \n",
    "# except pd.errors.ParserError:\n",
    "#     print(\"Your data contained rows that could not be parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f9a29cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (Temp/ipykernel_19548/1702412426.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\SAHEED~1\\AppData\\Local\\Temp/ipykernel_19548/1702412426.py\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    #     print(\"Your data contained rows that could not be parsed.\")\u001b[0m\n\u001b[1;37m                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  # Set warn_bad_lines to issue warnings about bad records\n",
    "#   data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\", \n",
    "#                      error_bad_lines=False, \n",
    "#                      warn_bad_lines = True)\n",
    "  \n",
    "#   # View first 5 records\n",
    "  print(data.head())\n",
    "  \n",
    "# except pd.errors.ParserError:\n",
    "#     print(\"Your data contained rows that could not be parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c418391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Read spreadsheet and assign it to survey_responses\n",
    "survey_responses = pd.read_excel('fcc-new-coder-survey-data Engineering.xlsx')\n",
    "\n",
    "# View the head of the dataframe\n",
    "print(survey_responses.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a76c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create string of lettered columns to load\n",
    "col_string = 'AD,AW:BA'\n",
    "\n",
    "# Load data with skiprows and usecols set\n",
    "survey_responses = pd.read_excel(\"fcc-new-coder-survey-data Engineering.xlsx\", \n",
    "                        usecols=col_string,skiprows=2)\n",
    "\n",
    "# View the names of the columns selected\n",
    "print(survey_responses.columns)\n",
    "print(survey_responses.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f225c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_responses = pd.read_excel(\"fcc-new-coder-survey-data Engineering.xlsx\")\n",
    "survey_responses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4889bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df from second worksheet by referencing its position\n",
    "responses_2017 = pd.read_excel(\"fcc-new-coder-survey-data Engineering.xlsx\",\n",
    "                               sheet_name=1)\n",
    "responses_2017.head()\n",
    "# # Graph where people would like to get a developer job\n",
    "# job_prefs = responses_2017.groupby(\"JobPref\").JobPref.count()\n",
    "# job_prefs.plot.barh()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55db9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create df from second worksheet by referencing its name\n",
    "# responses_2017 = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "#                                sheet_name='2017')\n",
    "\n",
    "# # Graph where people would like to get a developer job\n",
    "# job_prefs = responses_2017.groupby(\"JobPref\").JobPref.count()\n",
    "# job_prefs.plot.barh()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d048976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load both the 2016 and 2017 sheets by name\n",
    "# all_survey_data = pd.read_excel(\"fcc-new-coder-survey-data Engineering.xlsx\",\n",
    "#                                 sheet_name=['2016','2017'])\n",
    "\n",
    "# # View the data type of all_survey_data\n",
    "# print(type(all_survey_data))\n",
    "# print(all_survey_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all sheets in the Excel file\n",
    "all_survey_data = pd.read_excel(\"fcc-new-coder-survey-data Engineering.xlsx\",\n",
    "                                sheet_name=[0,'2017'])\n",
    "\n",
    "# View the sheet names in all_survey_data\n",
    "print(all_survey_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e815d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all sheets in the Excel file\n",
    "all_survey_data = pd.read_excel(\"fcc-new-coder-survey-data Engineering.xlsx\",\n",
    "                                sheet_name=None)\n",
    "\n",
    "# View the sheet names in all_survey_data\n",
    "print(all_survey_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty dataframe\n",
    "# all_responses = pd.DataFrame()\n",
    "\n",
    "# # Set up for loop to iterate through values in responses\n",
    "# for df in responses.values():\n",
    "#   # Print the number of rows being added\n",
    "#   print(\"Adding {} rows\".format(df.shape[0]))\n",
    "#   # Append df to all_responses, assign result\n",
    "#   all_responses = all_responses.append(df)\n",
    "\n",
    "# # Graph employment statuses in sample\n",
    "# counts = all_responses.groupby(\"EmploymentStatus\").EmploymentStatus.count()\n",
    "# counts.plot.barh()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586c4712",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data = pd.read_excel(\"fcc-new-coder-survey-data Engineering.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e11f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978839d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dtype to load appropriate column(s) as Boolean data\n",
    "# survey_data = pd.read_excel('fcc_survey_subset.xlsx')\n",
    "# survey_data.head()\n",
    "# survey_data = pd.read_excel(\"fcc_survey_subset.xlsx\",\n",
    "#                             dtype = {'HasDebt':bool})\n",
    "\n",
    "# View financial burdens by Boolean group\n",
    "# print(survey_data.groupby('HasDebt').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d46f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file with Yes as a True value and No as a False value\n",
    "# survey_subset = pd.read_excel(\"fcc_survey_yn_data.xlsx\",\n",
    "#                               dtype={\"HasDebt\": bool,\n",
    "#                               \"AttendedBootCampYesNo\": bool},\n",
    "#                               true_values=['Yes'],\n",
    "#                               false_values=['No'])\n",
    "\n",
    "# # View the data\n",
    "# print(survey_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1eeecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load file, with Part1StartTime parsed as datetime data\n",
    "# survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "#                             parse_dates=['Part1StartTime'])\n",
    "\n",
    "# # Print first few values of Part1StartTime\n",
    "# print(survey_data.Part1StartTime.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fded281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dict of columns to combine into new datetime column\n",
    "# datetime_cols = {\"Part2Start\": ['Part2StartDate','Part2StartTime']}\n",
    "\n",
    "\n",
    "# # Load file, supplying the dict to parse_dates\n",
    "# survey_data = pd.read_excel(\"fcc_survey_dts.xlsx\",\n",
    "#                             parse_dates=datetime_cols)\n",
    "\n",
    "# # View summary statistics about Part2Start\n",
    "# print(survey_data.Part2Start.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22a547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parse datetimes and assign result back to Part2EndTime\n",
    "# survey_data[\"Part2EndTime\"] = pd.to_datetime(survey_data[\"Part2EndTime\"], \n",
    "#                                              format=\"%m%d%Y %H:%M:%S\")\n",
    "\n",
    "# # Print first few values of Part2EndTime\n",
    "# print(survey_data.Part2EndTime.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///fcc-new-coder-survey-data Engineering.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3173f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine.fcc-new-coder-survey-data Engineering_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b0c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load libraries\n",
    "# import pandas as pd\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "# # Create the database engine\n",
    "# engine = create_engine('sqlite:///data.db')\n",
    "\n",
    "# # Load hpd311calls without any SQL\n",
    "# hpd_calls = pd.read_sql('hpd311calls', engine)\n",
    "\n",
    "# # View the first few rows of data\n",
    "# print(hpd_calls.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756fc1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the database engine\n",
    "# engine = create_engine(\"sqlite:///data.db\")\n",
    "\n",
    "# # Create a SQL query to load the entire weather table\n",
    "# query = \"\"\"\n",
    "# SELECT *\n",
    "#   FROM weather;\n",
    "# \"\"\"\n",
    "\n",
    "# # Load weather with the SQL query\n",
    "# weather = pd.read_sql(query, engine)\n",
    "\n",
    "# # View the first few rows of data\n",
    "# print(weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa88c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create database engine for data.db\n",
    "# engine = create_engine('sqlite:///data.db')\n",
    "\n",
    "# # Write query to get date, tmax, and tmin from weather\n",
    "# query = \"\"\"\n",
    "# SELECT date, \n",
    "#        tmax, \n",
    "#        tmin\n",
    "#   FROM weather;\n",
    "# \"\"\"\n",
    "\n",
    "# # Make a dataframe by passing query and engine to read_sql()\n",
    "# temperatures = pd.read_sql(query,engine)\n",
    "\n",
    "# # View the resulting dataframe\n",
    "# print(temperatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12181a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create query to get hpd311calls records about safety\n",
    "# query = \"\"\"\n",
    "# SELECT *\n",
    "# from hpd311calls\n",
    "# where complaint_type = 'SAFETY';\n",
    "# \"\"\"\n",
    "\n",
    "# # Query the database and assign result to safety_calls\n",
    "# safety_calls = pd.read_sql(query,engine)\n",
    "\n",
    "# # Graph the number of safety calls by borough\n",
    "# call_counts = safety_calls.groupby('borough').unique_key.count()\n",
    "# call_counts.plot.barh()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66383c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create query for records with max temps <= 32 or snow >= 1\n",
    "# query = \"\"\"\n",
    "# SELECT *\n",
    "#   FROM weather\n",
    "#   where tmax <=32\n",
    "#   or snow >=1;\n",
    "# \"\"\"\n",
    "\n",
    "# # Query database and assign result to wintry_days\n",
    "# wintry_days = pd.read_sql(query,engine)\n",
    "\n",
    "# # View summary stats about the temperatures\n",
    "# print(wintry_days.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30720656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create query for unique combinations of borough and complaint_type\n",
    "# query = \"\"\"\n",
    "# SELECT distinct borough, \n",
    "#        complaint_type\n",
    "#   from hpd311calls;\n",
    "# \"\"\"\n",
    "\n",
    "# # Load results of query to a dataframe\n",
    "# issues_and_boros = pd.read_sql(query,engine)\n",
    "\n",
    "# # Check assumption about issues and boroughs\n",
    "# print(issues_and_boros.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create query to get call counts by complaint_type\n",
    "# query = \"\"\"\n",
    "# select complaint_type, \n",
    "#      count(*)\n",
    "#   FROM hpd311calls\n",
    "#   group by complaint_type;\n",
    "# \"\"\"\n",
    "\n",
    "# # Create dataframe of call counts by issue\n",
    "# calls_by_issue = pd.read_sql(query, engine)\n",
    "\n",
    "# # Graph the number of calls for each housing issue\n",
    "# calls_by_issue.plot.barh(x=\"complaint_type\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0f1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a query to get month, max tmax, and min tmin by month\n",
    "# query = \"\"\"\n",
    "# SELECT month, \n",
    "# \t   MAX(tmax), \n",
    "#        MIN(tmin)\n",
    "#   FROM weather \n",
    "#  GROUP BY month;\n",
    "# \"\"\"\n",
    "\n",
    "# # Get dataframe of monthly weather stats\n",
    "# weather_by_month = pd.read_sql(query, engine)\n",
    "\n",
    "# # View weather stats by month\n",
    "# print(weather_by_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6853982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Query to join weather to call records by date columns\n",
    "# query = \"\"\"\n",
    "# SELECT * \n",
    "#   FROM hpd311calls\n",
    "#   JOIN weather \n",
    "#   ON hpd311calls.created_date = weather.date;\n",
    "# \"\"\"\n",
    "\n",
    "# # Create dataframe of joined tables\n",
    "# calls_with_weather = pd.read_sql(query, engine)\n",
    "\n",
    "# # View the dataframe to make sure all columns were joined\n",
    "# print(calls_with_weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cae01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Query to get hpd311calls and precipitation values\n",
    "# query = \"\"\"\n",
    "# SELECT hpd311calls.*, weather.prcp\n",
    "#   FROM hpd311calls\n",
    "#   JOIN weather\n",
    "#     ON hpd311calls.created_date = weather.date;\"\"\"\n",
    "\n",
    "# # Load query results into the leak_calls dataframe\n",
    "# leak_calls = pd.read_sql(query, engine)\n",
    "\n",
    "# # View the dataframe\n",
    "# print(leak_calls.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763749dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Query to get water leak calls and daily precipitation\n",
    "# query = \"\"\"\n",
    "# SELECT hpd311calls.*, weather.prcp\n",
    "#   FROM hpd311calls\n",
    "#   JOIN weather\n",
    "#     ON hpd311calls.created_date = weather.date\n",
    "#   where hpd311calls.complaint_type = 'WATER LEAK';\"\"\"\n",
    "\n",
    "# # Load query results into the leak_calls dataframe\n",
    "# leak_calls = pd.read_sql(query, engine)\n",
    "\n",
    "# # View the dataframe\n",
    "# print(leak_calls.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252389f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Query to get heat/hot water call counts by created_date\n",
    "# query = \"\"\"\n",
    "# SELECT hpd311calls.created_date, \n",
    "#        COUNT(*)\n",
    "#   FROM hpd311calls \n",
    "#  WHERE hpd311calls.complaint_type = 'HEAT/HOT WATER' \n",
    "#  GROUP BY hpd311calls.created_date;\n",
    "# \"\"\"\n",
    "\n",
    "# # Query database and save results as df\n",
    "# df = pd.read_sql(query, engine)\n",
    "\n",
    "# # View first 5 records\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab60c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modify query to join tmax and tmin from weather by date\n",
    "# query = \"\"\"\n",
    "# SELECT hpd311calls.created_date, \n",
    "# \t   COUNT(*), \n",
    "#        weather.tmax,\n",
    "#        weather.tmin\n",
    "#   FROM hpd311calls \n",
    "#        join weather\n",
    "#        on hpd311calls.created_date = weather.date\n",
    "#  WHERE hpd311calls.complaint_type = 'HEAT/HOT WATER' \n",
    "#  GROUP BY hpd311calls.created_date;\n",
    "#  \"\"\"\n",
    "\n",
    "# # Query database and save results as df\n",
    "# df = pd.read_sql(query, engine)\n",
    "\n",
    "# # View first 5 records\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e9eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load pandas as pd\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the daily report to a dataframe\n",
    "# pop_in_shelters = pd.read_json('dhs_daily_report.json')\n",
    "\n",
    "# # View summary stats about pop_in_shelters\n",
    "# print(pop_in_shelters.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b189c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     # Load the JSON with orient specified\n",
    "#     df = pd.read_json(\"dhs_report_reformatted.json\",\n",
    "#                       orient='split')\n",
    "    \n",
    "#     # Plot total population in shelters over time\n",
    "#     df[\"date_of_census\"] = pd.to_datetime(df[\"date_of_census\"])\n",
    "#     df.plot(x=\"date_of_census\", \n",
    "#             y=\"total_individuals_in_shelter\")\n",
    "#     plt.show()\n",
    "    \n",
    "# except ValueError:\n",
    "#     print(\"pandas could not parse the JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process of getting data from API\n",
    "#send and get data from API\n",
    "# not tied to a particular API\n",
    "# requests.get() to getr data from a url-requests.get(url_string)\n",
    "# Keyword arguement are: Params:takes a dictionary of parameter and values to customize API request\n",
    "# header: take a dictionary of value in providing user authentication to API\n",
    "# response.json() will return the json data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dafdef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'terms':'Bookstore','location':'Francesisco'}\n",
    "# header = {'Authorization','appl_key'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b746ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = request.get(api_url, params=params,headers=headers)\n",
    "# data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549bb2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "# params = {'terms':'Bookstore','location':'Francesisco'}\n",
    "# headers = {'Authorization':api_url}\n",
    "\n",
    "# # Get data about NYC cafes from the Yelp API\n",
    "# response = requests.get(api_url, \n",
    "#                 headers=headers, \n",
    "#                 params=params)\n",
    "\n",
    "# # Extract JSON data from the response\n",
    "# data = response.json()\n",
    "# data\n",
    "\n",
    "# # Load data to a dataframe\n",
    "# cafes = pd.DataFrame(data['businesses'])\n",
    "\n",
    "#  # View the data's dtypes\n",
    "# print(cafes.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23ad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dictionary to query API for cafes in NYC\n",
    "# parameters = {\"term\": \"cafe\",\n",
    "#           \t  \"location\": \"NYC\"}\n",
    "\n",
    "# # Query the Yelp API with headers and params set\n",
    "# response = requests.get(api_url, \n",
    "#                         headers=headers, \n",
    "#                         params=parameters)\n",
    "\n",
    "# # Extract JSON data from response\n",
    "# data = response.json()\n",
    "\n",
    "# # Load \"businesses\" values to a dataframe and print head\n",
    "# cafes = pd.DataFrame(data[\"businesses\"])\n",
    "# print(cafes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a0148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dictionary that passes Authorization and key string\n",
    "# headers = {'Authorization': \"Bearer {}\".format(api_key)}\n",
    "\n",
    "# # Query the Yelp API with headers and params set\n",
    "# response = requests.get(api_url,params=params,headers=headers)\n",
    "\n",
    "\n",
    "\n",
    "# # Extract JSON data from response\n",
    "# data = response.json()\n",
    "\n",
    "# # Load \"businesses\" values to a dataframe and print names\n",
    "# cafes = pd.DataFrame(data['businesses'])\n",
    "# print(cafes.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ac299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.io.json import json_normalize\n",
    "# # Load json_normalize()\n",
    "# from pandas.io.json import json_normalize\n",
    "\n",
    "# # Isolate the JSON data from the API response\n",
    "# data = response.json()\n",
    "\n",
    "# # Flatten business data into a dataframe, replace separator\n",
    "# cafes = json_normalize(data[\"businesses\"],\n",
    "#              sep = \"_\")\n",
    "\n",
    "# # View data\n",
    "# print(cafes.head())\n",
    "\n",
    "# # Specify record path to get categories data\n",
    "# flat_cafes = json_normalize(data[\"businesses\"],\n",
    "#                             sep=\"_\",\n",
    "#                     \t\trecord_path='categories')\n",
    "\n",
    "# # View the data\n",
    "# print(flat_cafes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5585de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load other business attributes and set meta prefix\n",
    "# flat_cafes = json_normalize(data[\"businesses\"],\n",
    "#                             sep=\"_\",\n",
    "#                     \t\trecord_path=\"categories\",\n",
    "#                     \t\tmeta=['name', \n",
    "#                                   'alias',  \n",
    "#                                   'rating',\n",
    "#                           \t\t  ['coordinates', 'latitude'], \n",
    "#                           \t\t  ['coordinates', 'longitude']],\n",
    "#                     \t\tmeta_prefix='biz_')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # View the data\n",
    "# print(flat_cafes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2776eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add an offset parameter to get cafes 51-100\n",
    "# params = {\"term\": \"cafe\", \n",
    "#           \"location\": \"NYC\",\n",
    "#           \"sort_by\": \"rating\", \n",
    "#           \"limit\": 50,\n",
    "#           'offset':50}\n",
    "\n",
    "# result = requests.get(api_url, headers=headers, params=params)\n",
    "# next_50_cafes = json_normalize(result.json()[\"businesses\"])\n",
    "\n",
    "# # Append the results, setting ignore_index to renumber rows\n",
    "# cafes = top_50_cafes.append(next_50_cafes,ignore_index=True)\n",
    "\n",
    "# # Print shape of cafes\n",
    "# print(cafes.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
