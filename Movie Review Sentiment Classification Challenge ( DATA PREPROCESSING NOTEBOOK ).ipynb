{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ7v3acufwwU"
   },
   "source": [
    "# Movie Review Sentiment Classification Challenge\n",
    "# **Author: MICADEE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOQU9duIfqoA"
   },
   "source": [
    "**Download already saved datasets from gdrive to colab using \"gdown\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKQQP8rvfmeF"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!gdown https://drive.google.com/uc?id=1k9Mco0zM4J1KX4yUO0nWXBcffkfVD0wy\n",
    "!unzip -qq Movies_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwee-9uM7i1K"
   },
   "source": [
    "**Install all necessary libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lQfcwWjZ7CL-"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -qq emoji==1.6.3 --quiet\n",
    "!pip install -qq transformers --quiet\n",
    "!pip install -qq matplotlib==3.4 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8ICYbpG7Itx"
   },
   "source": [
    "## **Note**:\n",
    "**To avoid any error inside this notebook, make sure and confirm that the package version of matplotlib below is excatly 3.4, otherwise restart and rerun this colab notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4XM8NC47GxZ"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnQxtwOXiw2c"
   },
   "source": [
    "Great !!! we have mayplotlib version of exactly 3.4.\n",
    "\n",
    "Here we go !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FR24B33t0dgl"
   },
   "source": [
    "# STAGE 1:\n",
    "# DATA CLEANING PROCEDURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTXVtnCW7boU"
   },
   "source": [
    "**Import all necessary libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgxCS6lt0cwY"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import emoji\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import re, string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import sklearn.exceptions\n",
    "from pylab import rcParams\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from transformers import TFBertModel\n",
    "from transformers import TFRobertaModel\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import RobertaTokenizerFast\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split , KFold , StratifiedKFold\n",
    "from transformers import BertModel, BertTokenizer, AdamW,  get_linear_schedule_with_warmup, set_seed\n",
    "\n",
    "#set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lo2_ERcd7Upg"
   },
   "source": [
    "**Set seed for reproducibility:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6TYTkDWclkX"
   },
   "outputs": [],
   "source": [
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed =42\n",
    "seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAgWwZL6741V"
   },
   "source": [
    "**Loading the dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKxhP4R97w99"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Train.csv\")\n",
    "df_test = pd.read_csv(\"Test.csv\")\n",
    "sub = pd.read_csv(\"SampleSubmission.csv\")\n",
    "\n",
    "print(df.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ttn0mI_K8DbE"
   },
   "source": [
    "NOTE: UTF-8 encoding does not work on the dataset when loading it with pandas 'read_csv' function. This lead to the use of 'ISO-8859-1'/latin-1 encoding. <br>\n",
    "It will be found later that some special characters like apostrophes are turned into '\\x92', which will be taken care of during the data cleaning process.\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGjywv2mcL3z"
   },
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPZQNNYacCIX"
   },
   "outputs": [],
   "source": [
    "sentiment_map = {'negative':0,'positive':1}\n",
    "df['sentiment'] = df['sentiment'].map(sentiment_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hA8Pz2WZ7w7k"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKtedtR07w43"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJCYsoGD8MV8"
   },
   "source": [
    "## Check if there's duplicated tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfMwbMqb7w13"
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset='content',inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjp3at5R8YQg"
   },
   "source": [
    "Good news, there are few duplicate tweets !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU7baG-a8hut"
   },
   "source": [
    "# Tweets Deep Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKQff9HU8dcp"
   },
   "source": [
    "In the following, we will perform some data cleaning on the raw text of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezgyKuzE7wzY"
   },
   "outputs": [],
   "source": [
    "df = df[['review_file', 'content',\t'sentiment']]\n",
    "df_test = df_test[['review_file','content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_yQlzeM8vyf"
   },
   "source": [
    "#DEFINE CUSTOM FUNCTIONS TO CLEAN THE TEXT OF THE TWEETS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bq5_sL_Y7wwk"
   },
   "outputs": [],
   "source": [
    "# Clean emojis from text\n",
    "def strip_emoji(text):\n",
    "    return re.sub(emoji.get_emoji_regexp(), r\"\", text)   # remove emoji\n",
    "\n",
    "# Remove punctuations, links, mentions and \\r\\n new line characters\n",
    "def strip_all_entities(text):\n",
    "    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) #remove links and mentions\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n",
    "    banned_list= string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'\n",
    "    table = str.maketrans('', '', banned_list)\n",
    "    text = text.translate(table)\n",
    "    return text\n",
    "\n",
    "# Clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\n",
    "def clean_hashtags(tweet):\n",
    "    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n",
    "    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n",
    "    return new_tweet2\n",
    "\n",
    "# Filter special characters such as & and $ present in some words\n",
    "def filter_chars(a):\n",
    "    sent = []\n",
    "    for word in a.split(' '):\n",
    "        if ('$' in word) | ('&' in word):\n",
    "            sent.append('')\n",
    "        else:\n",
    "            sent.append(word)\n",
    "    return ' '.join(sent)\n",
    "\n",
    "def remove_mult_spaces(text): # remove multiple spaces\n",
    "    return re.sub(\"\\s\\s+\" , \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XfprweK7wtt"
   },
   "outputs": [],
   "source": [
    "texts_new = []\n",
    "for t in df.content:\n",
    "    texts_new.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crpuFjcS7wqi"
   },
   "outputs": [],
   "source": [
    "texts_new_test = []\n",
    "for t in df_test.content:\n",
    "    texts_new_test.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTdMyW_t9eQ6"
   },
   "source": [
    "Now we can create a new column, for both train and test sets, to host the cleaned version of the tweets' text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fy9lO9A9V6V"
   },
   "outputs": [],
   "source": [
    "df['text_clean'] = texts_new\n",
    "df_test['text_clean'] = texts_new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hN5MESmv9V3r"
   },
   "outputs": [],
   "source": [
    "df['text_clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSVBzSq99V1C"
   },
   "outputs": [],
   "source": [
    "df_test['text_clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmK3VIAc9VwD"
   },
   "outputs": [],
   "source": [
    "df['text_clean'][1:8].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Toirst09s7a"
   },
   "source": [
    "Moreover, we will also create a column to host the lenght of the cleaned text, to check if by cleaning the text we removed too much text or almost entirely the tweet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCXsIMp49Vs_"
   },
   "outputs": [],
   "source": [
    "text_len = []\n",
    "for text in df.text_clean:\n",
    "    tweet_len = len(text.split())\n",
    "    text_len.append(tweet_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qOZJLqg9p4U"
   },
   "outputs": [],
   "source": [
    "df['text_len'] = text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVWSeQf49p1V"
   },
   "outputs": [],
   "source": [
    "text_len_test = []\n",
    "for text in df_test.text_clean:\n",
    "    tweet_len = len(text.split())\n",
    "    text_len_test.append(tweet_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CY4x5J7I9pyn"
   },
   "outputs": [],
   "source": [
    "df_test['text_len'] = text_len_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRRtJDhJ9pvo"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "ax = sns.countplot(x='text_len', data=df[df['text_len']<30], palette='mako')\n",
    "plt.title('Training tweets with less than 30 words')\n",
    "plt.yticks([])\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PjLDAlO9ps5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "ax = sns.countplot(x='text_len', data=df_test[df_test['text_len']<30], palette='mako')\n",
    "plt.title('Training tweets with less than 30 words')\n",
    "plt.yticks([])\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_m5z8Yd9_9N"
   },
   "source": [
    "As we can see, there are lots of cleaned tweets (2 precisely) with just ten words inside train dataset: this is due to the cleaning performed before. This means that some tweets contained only mentions, hashtags and links, which have been removed. We will drop these empty tweets and also those with less than 1 word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cK8vvfS9pqX"
   },
   "outputs": [],
   "source": [
    "print(f\" DF SHAPE: {df.shape}\")\n",
    "print(f\" DF TEST SHAPE: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EH5Mp_sq9pnF"
   },
   "outputs": [],
   "source": [
    "df = df[df['text_len'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mLhokJQ9pke"
   },
   "outputs": [],
   "source": [
    "print(f\" DF SHAPE: {df.shape}\")\n",
    "print(f\" DF TEST SHAPE: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzReZ9my-Olb"
   },
   "source": [
    "# STAGE 1(A):\n",
    "# Training data deeper cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pM6wZo-o-kCH"
   },
   "source": [
    "Let's perform a further cleaning checking the tokenizer version of the sentences.\n",
    "\n",
    "First, we import the BERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoNbB88t9phz"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DSVRFKl9pfB"
   },
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "\n",
    "for txt in df['text_clean'].values:\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))\n",
    "\n",
    "max_len=np.max(token_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57Top1Mv9pcK"
   },
   "outputs": [],
   "source": [
    "print(f\"MAX TOKENIZED SENTENCE LENGTH: {max_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDpqXKP1-347"
   },
   "source": [
    "Let's check the long tokenized sentences (with more than 40 tokens ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTb_Z3hc-1V2"
   },
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "\n",
    "for i,txt in enumerate(df['text_clean'].values):\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))\n",
    "    if len(tokens)>400:\n",
    "        print(f\"INDEX: {i}, TEXT: {txt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eR6XfGJ-1TJ"
   },
   "outputs": [],
   "source": [
    "df['token_lens'] = token_lens\n",
    "\n",
    "df = df.sort_values(by='token_lens', ascending=False)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89aL1QCP-1QO"
   },
   "outputs": [],
   "source": [
    "df = df.iloc[12:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxh5innj_Pkl"
   },
   "source": [
    "The dataset looks more clean now. We will shuffle it and reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59N9zvWR-1NZ"
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIF_8yZi_U_J"
   },
   "source": [
    "# STAGE 1(B):\n",
    "## Test data deeper cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoQ8Rqvl_eiB"
   },
   "source": [
    "We will perform the data cleaning based on the tokenized sentences on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJgwaZfV-1Kb"
   },
   "outputs": [],
   "source": [
    "token_lens_test = []\n",
    "\n",
    "for txt in df_test['text_clean'].values:\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens_test.append(len(tokens))\n",
    "\n",
    "max_len=np.max(token_lens_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiUQILB1-1H9"
   },
   "outputs": [],
   "source": [
    "print(f\"MAX TOKENIZED SENTENCE LENGTH: {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jI_iRem1-1FS"
   },
   "outputs": [],
   "source": [
    "token_lens_test = []\n",
    "\n",
    "for i,txt in enumerate(df_test['text_clean'].values):\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens_test.append(len(tokens))\n",
    "    if len(tokens)>400:\n",
    "        print(f\"INDEX: {i}, TEXT: {txt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nu5ZvPn7-1Cm"
   },
   "outputs": [],
   "source": [
    "df_test['token_lens'] = token_lens_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HomNbI6d-0_7"
   },
   "outputs": [],
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZOFBve8-08q"
   },
   "outputs": [],
   "source": [
    "df.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8Kw-00X_0zK"
   },
   "source": [
    "Now the data cleaning is completed.\n",
    "\n",
    "# Let's save the final preprocessed datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahXdlF509pWN"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"train_preprocessed.csv\", index = False)\n",
    "df_test.to_csv(\"test_preprocessed.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_CNDBCbtOIZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMn-b855tOEm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCxKmpn5ENO6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSzNHTs3ENMI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
