{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353d0663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saheediyanda\\Anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize,regexp_tokenize,TweetTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b19c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>✅ Trip Verified |  I have come to boarding and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>✅ Trip Verified | Stinking nappies being chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>✅ Trip Verified | Worst service ever. Lost bag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>✅ Trip Verified |  BA 246 21JAN 2023 Did not a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>✅ Trip Verified | Not a great experience. I co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            reviews\n",
       "0           0  ✅ Trip Verified |  I have come to boarding and...\n",
       "1           1  ✅ Trip Verified | Stinking nappies being chang...\n",
       "2           2  ✅ Trip Verified | Worst service ever. Lost bag...\n",
       "3           3  ✅ Trip Verified |  BA 246 21JAN 2023 Did not a...\n",
       "4           4  ✅ Trip Verified | Not a great experience. I co..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('BA_Review')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fffacbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3742, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a8430b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['verified?'] = data['reviews'].apply(lambda x: x.split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af227cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviews</th>\n",
       "      <th>verified?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>✅ Trip Verified |  I have come to boarding and...</td>\n",
       "      <td>✅ Trip Verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>✅ Trip Verified | Stinking nappies being chang...</td>\n",
       "      <td>✅ Trip Verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>✅ Trip Verified | Worst service ever. Lost bag...</td>\n",
       "      <td>✅ Trip Verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>✅ Trip Verified |  BA 246 21JAN 2023 Did not a...</td>\n",
       "      <td>✅ Trip Verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>✅ Trip Verified | Not a great experience. I co...</td>\n",
       "      <td>✅ Trip Verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>3737</td>\n",
       "      <td>Flew LHR - VIE return operated by bmi but BA a...</td>\n",
       "      <td>Flew LHR - VIE return operated by bmi but BA a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>3738</td>\n",
       "      <td>LHR to HAM. Purser addresses all club passenge...</td>\n",
       "      <td>LHR to HAM. Purser addresses all club passenge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>3739</td>\n",
       "      <td>My son who had worked for British Airways urge...</td>\n",
       "      <td>My son who had worked for British Airways urge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3740</th>\n",
       "      <td>3740</td>\n",
       "      <td>London City-New York JFK via Shannon on A318 b...</td>\n",
       "      <td>London City-New York JFK via Shannon on A318 b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>3741</td>\n",
       "      <td>SIN-LHR BA12 B747-436 First Class. Old aircraf...</td>\n",
       "      <td>SIN-LHR BA12 B747-436 First Class. Old aircraf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3742 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                            reviews  \\\n",
       "0              0  ✅ Trip Verified |  I have come to boarding and...   \n",
       "1              1  ✅ Trip Verified | Stinking nappies being chang...   \n",
       "2              2  ✅ Trip Verified | Worst service ever. Lost bag...   \n",
       "3              3  ✅ Trip Verified |  BA 246 21JAN 2023 Did not a...   \n",
       "4              4  ✅ Trip Verified | Not a great experience. I co...   \n",
       "...          ...                                                ...   \n",
       "3737        3737  Flew LHR - VIE return operated by bmi but BA a...   \n",
       "3738        3738  LHR to HAM. Purser addresses all club passenge...   \n",
       "3739        3739  My son who had worked for British Airways urge...   \n",
       "3740        3740  London City-New York JFK via Shannon on A318 b...   \n",
       "3741        3741  SIN-LHR BA12 B747-436 First Class. Old aircraf...   \n",
       "\n",
       "                                              verified?  \n",
       "0                                      ✅ Trip Verified   \n",
       "1                                      ✅ Trip Verified   \n",
       "2                                      ✅ Trip Verified   \n",
       "3                                      ✅ Trip Verified   \n",
       "4                                      ✅ Trip Verified   \n",
       "...                                                 ...  \n",
       "3737  Flew LHR - VIE return operated by bmi but BA a...  \n",
       "3738  LHR to HAM. Purser addresses all club passenge...  \n",
       "3739  My son who had worked for British Airways urge...  \n",
       "3740  London City-New York JFK via Shannon on A318 b...  \n",
       "3741  SIN-LHR BA12 B747-436 First Class. Old aircraf...  \n",
       "\n",
       "[3742 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f280a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['verified?'] != '✅ Trip Verified ','verified?'] = 'Not Verified'\n",
    "data.loc[data['verified?'] == '✅ Trip Verified ','verified?'] = 'Trip Verified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77395978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviews</th>\n",
       "      <th>verified?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>✅ Trip Verified |  I have come to boarding and...</td>\n",
       "      <td>Trip Verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>✅ Trip Verified | Stinking nappies being chang...</td>\n",
       "      <td>Trip Verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>✅ Trip Verified | Worst service ever. Lost bag...</td>\n",
       "      <td>Trip Verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>✅ Trip Verified |  BA 246 21JAN 2023 Did not a...</td>\n",
       "      <td>Trip Verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>✅ Trip Verified | Not a great experience. I co...</td>\n",
       "      <td>Trip Verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>3737</td>\n",
       "      <td>Flew LHR - VIE return operated by bmi but BA a...</td>\n",
       "      <td>Not Verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>3738</td>\n",
       "      <td>LHR to HAM. Purser addresses all club passenge...</td>\n",
       "      <td>Not Verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>3739</td>\n",
       "      <td>My son who had worked for British Airways urge...</td>\n",
       "      <td>Not Verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3740</th>\n",
       "      <td>3740</td>\n",
       "      <td>London City-New York JFK via Shannon on A318 b...</td>\n",
       "      <td>Not Verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>3741</td>\n",
       "      <td>SIN-LHR BA12 B747-436 First Class. Old aircraf...</td>\n",
       "      <td>Not Verified</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3742 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                            reviews  \\\n",
       "0              0  ✅ Trip Verified |  I have come to boarding and...   \n",
       "1              1  ✅ Trip Verified | Stinking nappies being chang...   \n",
       "2              2  ✅ Trip Verified | Worst service ever. Lost bag...   \n",
       "3              3  ✅ Trip Verified |  BA 246 21JAN 2023 Did not a...   \n",
       "4              4  ✅ Trip Verified | Not a great experience. I co...   \n",
       "...          ...                                                ...   \n",
       "3737        3737  Flew LHR - VIE return operated by bmi but BA a...   \n",
       "3738        3738  LHR to HAM. Purser addresses all club passenge...   \n",
       "3739        3739  My son who had worked for British Airways urge...   \n",
       "3740        3740  London City-New York JFK via Shannon on A318 b...   \n",
       "3741        3741  SIN-LHR BA12 B747-436 First Class. Old aircraf...   \n",
       "\n",
       "          verified?  \n",
       "0     Trip Verified  \n",
       "1     Trip Verified  \n",
       "2     Trip Verified  \n",
       "3     Trip Verified  \n",
       "4     Trip Verified  \n",
       "...             ...  \n",
       "3737   Not Verified  \n",
       "3738   Not Verified  \n",
       "3739   Not Verified  \n",
       "3740   Not Verified  \n",
       "3741   Not Verified  \n",
       "\n",
       "[3742 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae26f8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'✅ Trip Verified |  I have come to boarding and my cabin luggage was taken, \"because the plane is full\". I asked to take it to the cabin, because I have large notebook and  electronics in it, but nothing changes. So, now I am sitting in the plane, holding notebook and several packs from my luggage, and think of how I will stand with all this stuff in my hands in a passport control line in Warsaw. Other people around me have come to the plane with cabin luggage!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['reviews'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e7cfd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"✅ Trip Verified |\\xa0\\xa0BA 246 21JAN 2023 Did not appreciate the unprofessional attitude of the pilots. Flight scheduled departure 16:20. Advised boarding time 15:20. Whole flight full of passengers waiting at the gate to board at 15:20. 15:40 the cabin crew board. 15:55 the pilots board - each with a Sao Paulo Airport Duty Free branded shopping bag. 16:20 the flight is still boarding. Finally pushes back just before 17:00, 40 minutes late. Captain came on the intercom to announce the delay was due to 'traffic between the crew hotel and the airport'. Sorry Captain the whole plane saw you and your pilot colleagues board fifteen minutes after the cabin crew clutching your duty free that you and your pilot colleagues still made time to stop for.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['reviews'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0347b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Not Verified     2558\n",
       "Trip Verified    1184\n",
       "Name: verified?, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the value count of the verified column\n",
    "data['verified?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "166c55f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk, spacy, gensim\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae242508",
   "metadata": {},
   "source": [
    "This code defines a function called `sent_to_words` that takes a list of sentences as input. The function iterates over each sentence in the input list and uses the `gensim.utils.simple_preprocess` function to process the sentence by converting it to lowercase and removing any punctuation. The processed sentence is then yielded back.\n",
    "\n",
    "The `data_words` variable is then assigned the result of calling the `sent_to_words` function with the 'review_text' column of the `data` dataframe as input. This will convert the sentences in the 'review_text' column into a list of words.\n",
    "\n",
    "Finally, the code prints the first element of the `data_words` list. This will display the processed words from the first sentence in the 'review_text' column of the `data` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7612186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['trip', 'verified', 'have', 'come', 'to', 'boarding', 'and', 'my', 'cabin', 'luggage', 'was', 'taken', 'because', 'the', 'plane', 'is', 'full', 'asked', 'to', 'take', 'it', 'to', 'the', 'cabin', 'because', 'have', 'large', 'notebook', 'and', 'electronics', 'in', 'it', 'but', 'nothing', 'changes', 'so', 'now', 'am', 'sitting', 'in', 'the', 'plane', 'holding', 'notebook', 'and', 'several', 'packs', 'from', 'my', 'luggage', 'and', 'think', 'of', 'how', 'will', 'stand', 'with', 'all', 'this', 'stuff', 'in', 'my', 'hands', 'in', 'passport', 'control', 'line', 'in', 'warsaw', 'other', 'people', 'around', 'me', 'have', 'come', 'to', 'the', 'plane', 'with', 'cabin', 'luggage']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data['reviews'].tolist()))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f1723",
   "metadata": {},
   "source": [
    "This code defines a function called `lemmatization` that takes a list of texts and an optional parameter `allowed_postags` as input. The `allowed_postags` parameter specifies the parts of speech that should be included in the lemmatization process and defaults to nouns, adjectives, verbs, and adverbs if not specified.\n",
    "\n",
    "Inside the function, a new list called `texts_out` is initialized. Then, for each text in the input list, the function uses the spaCy `nlp` object to process the text and tokenize it into words. The words are then lemmatized and appended to the `texts_out` list, based on the allowed parts of speech specified.\n",
    "\n",
    "Finally, the function returns the `texts_out` list containing the lemmatized texts.\n",
    "\n",
    "The lemmatization process involves converting words to their base or dictionary form, which can help in standardizing and normalizing the text data for further analysis or natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c776973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trip verify come boarding cabin luggage take plane full ask take cabin large notebook electronic change so now sit plane hold notebook several pack luggage think stand stuff hand passport control line other people come plane cabin luggage']\n"
     ]
    }
   ],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85e948a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b01b6",
   "metadata": {},
   "source": [
    "The code performs the following actions:\n",
    "\n",
    "1. It converts a sparse matrix `data_vectorized` into a dense matrix `data_dense` using the `todense()` method. In the context of natural language processing, this is often done after vectorizing text data using techniques like TF-IDF or Count Vectorization, which typically result in sparse matrices.\n",
    "\n",
    "2. After converting the matrix, the code calculates the sparsity of the matrix. Sparsity is a measure of how many elements in a matrix are zero. In this case, the code calculates the sparsity as the percentage of non-zero elements in the matrix. It does this by comparing each element in the dense matrix to zero, summing the number of non-zero elements, and then dividing by the total number of elements in the matrix.\n",
    "\n",
    "3. The result is printed, showing the sparsity as a percentage.\n",
    "\n",
    "In summary, the code first converts a sparse matrix to a dense matrix, and then calculates and prints the sparsity of the dense matrix. This kind of analysis is common in text processing and machine learning to understand the density of the data and to make decisions about the most appropriate algorithms and techniques to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f4c2880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsicity:  2.320316755299133 %\n"
     ]
    }
   ],
   "source": [
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d9b5068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00893255 0.26482893 0.71730119 0.00893733]\n",
      " [0.25440386 0.66780823 0.00142746 0.07636045]\n",
      " [0.82670977 0.01124806 0.15097765 0.01106452]\n",
      " ...\n",
      " [0.53624558 0.34885259 0.00417578 0.11072605]\n",
      " [0.01724364 0.01871156 0.01712368 0.94692112]\n",
      " [0.00512972 0.59978365 0.00520229 0.38988434]]\n"
     ]
    }
   ],
   "source": [
    "# Build LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components=4,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa258d1",
   "metadata": {},
   "source": [
    "This code is using the Latent Dirichlet Allocation (LDA) algorithm, which is a popular technique for topic modeling in natural language processing. Here's a breakdown of the code:\n",
    "\n",
    "1. `LatentDirichletAllocation` is a class from the `sklearn.decomposition` module that represents an LDA model. It takes several parameters:\n",
    "\n",
    "   - `n_components`: Specifies the number of topics to be identified in the data.\n",
    "   - `max_iter`: Sets the maximum number of iterations for the LDA algorithm to converge.\n",
    "   - `learning_method`: Determines the method used for learning the model. In this case, 'online' learning method is used.\n",
    "   - `random_state`: Sets the seed for the random number generator, ensuring reproducibility of the results.\n",
    "   - `batch_size`: Defines the number of documents to be used in each learning iteration.\n",
    "   - `evaluate_every`: Specifies how often the model should compute the perplexity. A value of -1 means the perplexity is not computed.\n",
    "   - `n_jobs`: Sets the number of CPU cores to use for parallelizing the computation. A value of -1 means all available CPU cores will be used.\n",
    "\n",
    "2. The `LatentDirichletAllocation` model is then fitted to the `data_vectorized` input, which is typically a document-term matrix obtained through techniques like TF-IDF or Count Vectorization. The `fit_transform` method applies the LDA model to the input data and returns the document-topic distribution.\n",
    "\n",
    "After running this code, `lda_output` will contain the document-topic matrix, where each row represents a document and each column represents a topic, with the values indicating the probability of the document belonging to that topic. This can be further analyzed to understand the topics present in the corpus of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45a090a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_9d7f0_row0_col0, #T_9d7f0_row0_col3, #T_9d7f0_row1_col2, #T_9d7f0_row1_col3, #T_9d7f0_row2_col1, #T_9d7f0_row2_col3, #T_9d7f0_row2_col4, #T_9d7f0_row3_col0, #T_9d7f0_row3_col1, #T_9d7f0_row3_col3, #T_9d7f0_row4_col0, #T_9d7f0_row4_col1, #T_9d7f0_row5_col0, #T_9d7f0_row6_col2, #T_9d7f0_row6_col3, #T_9d7f0_row6_col4, #T_9d7f0_row7_col0, #T_9d7f0_row7_col2, #T_9d7f0_row7_col3, #T_9d7f0_row8_col4, #T_9d7f0_row9_col1, #T_9d7f0_row9_col3, #T_9d7f0_row10_col1, #T_9d7f0_row12_col0, #T_9d7f0_row12_col1, #T_9d7f0_row13_col3, #T_9d7f0_row13_col4, #T_9d7f0_row14_col1, #T_9d7f0_row14_col3, #T_9d7f0_row14_col4 {\n",
       "  color: black;\n",
       "  font-weight: 400;\n",
       "}\n",
       "#T_9d7f0_row0_col1, #T_9d7f0_row0_col2, #T_9d7f0_row0_col4, #T_9d7f0_row1_col0, #T_9d7f0_row1_col1, #T_9d7f0_row1_col4, #T_9d7f0_row2_col0, #T_9d7f0_row2_col2, #T_9d7f0_row3_col2, #T_9d7f0_row3_col4, #T_9d7f0_row4_col2, #T_9d7f0_row4_col3, #T_9d7f0_row4_col4, #T_9d7f0_row5_col1, #T_9d7f0_row5_col2, #T_9d7f0_row5_col3, #T_9d7f0_row5_col4, #T_9d7f0_row6_col0, #T_9d7f0_row6_col1, #T_9d7f0_row7_col1, #T_9d7f0_row7_col4, #T_9d7f0_row8_col0, #T_9d7f0_row8_col1, #T_9d7f0_row8_col2, #T_9d7f0_row8_col3, #T_9d7f0_row9_col0, #T_9d7f0_row9_col2, #T_9d7f0_row9_col4, #T_9d7f0_row10_col0, #T_9d7f0_row10_col2, #T_9d7f0_row10_col3, #T_9d7f0_row10_col4, #T_9d7f0_row11_col0, #T_9d7f0_row11_col1, #T_9d7f0_row11_col2, #T_9d7f0_row11_col3, #T_9d7f0_row11_col4, #T_9d7f0_row12_col2, #T_9d7f0_row12_col3, #T_9d7f0_row12_col4, #T_9d7f0_row13_col0, #T_9d7f0_row13_col1, #T_9d7f0_row13_col2, #T_9d7f0_row14_col0, #T_9d7f0_row14_col2 {\n",
       "  color: green;\n",
       "  font-weight: 700;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_9d7f0_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Topic0</th>\n",
       "      <th class=\"col_heading level0 col1\" >Topic1</th>\n",
       "      <th class=\"col_heading level0 col2\" >Topic2</th>\n",
       "      <th class=\"col_heading level0 col3\" >Topic3</th>\n",
       "      <th class=\"col_heading level0 col4\" >dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9d7f0_level0_row0\" class=\"row_heading level0 row0\" >Doc0</th>\n",
       "      <td id=\"T_9d7f0_row0_col0\" class=\"data row0 col0\" >0.010000</td>\n",
       "      <td id=\"T_9d7f0_row0_col1\" class=\"data row0 col1\" >0.260000</td>\n",
       "      <td id=\"T_9d7f0_row0_col2\" class=\"data row0 col2\" >0.720000</td>\n",
       "      <td id=\"T_9d7f0_row0_col3\" class=\"data row0 col3\" >0.010000</td>\n",
       "      <td id=\"T_9d7f0_row0_col4\" class=\"data row0 col4\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d7f0_level0_row1\" class=\"row_heading level0 row1\" >Doc1</th>\n",
       "      <td id=\"T_9d7f0_row1_col0\" class=\"data row1 col0\" >0.250000</td>\n",
       "      <td id=\"T_9d7f0_row1_col1\" class=\"data row1 col1\" >0.670000</td>\n",
       "      <td id=\"T_9d7f0_row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
       "      <td id=\"T_9d7f0_row1_col3\" class=\"data row1 col3\" >0.080000</td>\n",
       "      <td id=\"T_9d7f0_row1_col4\" class=\"data row1 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d7f0_level0_row2\" class=\"row_heading level0 row2\" >Doc2</th>\n",
       "      <td id=\"T_9d7f0_row2_col0\" class=\"data row2 col0\" >0.830000</td>\n",
       "      <td id=\"T_9d7f0_row2_col1\" class=\"data row2 col1\" >0.010000</td>\n",
       "      <td id=\"T_9d7f0_row2_col2\" class=\"data row2 col2\" >0.150000</td>\n",
       "      <td id=\"T_9d7f0_row2_col3\" class=\"data row2 col3\" >0.010000</td>\n",
       "      <td id=\"T_9d7f0_row2_col4\" class=\"data row2 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d7f0_level0_row3\" class=\"row_heading level0 row3\" >Doc3</th>\n",
       "      <td id=\"T_9d7f0_row3_col0\" class=\"data row3 col0\" >0.000000</td>\n",
       "      <td id=\"T_9d7f0_row3_col1\" class=\"data row3 col1\" >0.000000</td>\n",
       "      <td id=\"T_9d7f0_row3_col2\" class=\"data row3 col2\" >0.990000</td>\n",
       "      <td id=\"T_9d7f0_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "      <td id=\"T_9d7f0_row3_col4\" class=\"data row3 col4\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d7f0_level0_row4\" class=\"row_heading level0 row4\" >Doc4</th>\n",
       "      <td id=\"T_9d7f0_row4_col0\" class=\"data row4 col0\" >0.010000</td>\n",
       "      <td id=\"T_9d7f0_row4_col1\" class=\"data row4 col1\" >0.010000</td>\n",
       "      <td id=\"T_9d7f0_row4_col2\" class=\"data row4 col2\" >0.760000</td>\n",
       "      <td id=\"T_9d7f0_row4_col3\" class=\"data row4 col3\" >0.230000</td>\n",
       "      <td id=\"T_9d7f0_row4_col4\" class=\"data row4 col4\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d7f0_level0_row5\" class=\"row_heading level0 row5\" >Doc5</th>\n",
       "      <td id=\"T_9d7f0_row5_col0\" class=\"data row5 col0\" >0.030000</td>\n",
       "      <td id=\"T_9d7f0_row5_col1\" class=\"data row5 col1\" >0.480000</td>\n",
       "      <td id=\"T_9d7f0_row5_col2\" class=\"data row5 col2\" >0.140000</td>\n",
       "      <td id=\"T_9d7f0_row5_col3\" class=\"data row5 col3\" >0.350000</td>\n",
       "      <td id=\"T_9d7f0_row5_col4\" class=\"data row5 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d7f0_level0_row6\" class=\"row_heading level0 row6\" >Doc6</th>\n",
       "      <td id=\"T_9d7f0_row6_col0\" class=\"data row6 col0\" >0.860000</td>\n",
       "      <td id=\"T_9d7f0_row6_col1\" class=\"data row6 col1\" >0.130000</td>\n",
       "      <td id=\"T_9d7f0_row6_col2\" class=\"data row6 col2\" >0.000000</td>\n",
       "      <td id=\"T_9d7f0_row6_col3\" class=\"data row6 col3\" >0.000000</td>\n",
       "      <td id=\"T_9d7f0_row6_col4\" class=\"data row6 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d7f0_level0_row7\" class=\"row_heading level0 row7\" >Doc7</th>\n",
       "      <td id=\"T_9d7f0_row7_col0\" class=\"data row7 col0\" >0.100000</td>\n",
       "      <td id=\"T_9d7f0_row7_col1\" class=\"data row7 col1\" >0.880000</td>\n",
       "      <td id=\"T_9d7f0_row7_col2\" class=\"data row7 col2\" >0.010000</td>\n",
       "      <td id=\"T_9d7f0_row7_col3\" class=\"data row7 col3\" >0.010000</td>\n",
       "      <td id=\"T_9d7f0_row7_col4\" class=\"data row7 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d7f0_level0_row8\" class=\"row_heading level0 row8\" >Doc8</th>\n",
       "      <td id=\"T_9d7f0_row8_col0\" class=\"data row8 col0\" >0.490000</td>\n",
       "      <td id=\"T_9d7f0_row8_col1\" class=\"data row8 col1\" >0.150000</td>\n",
       "      <td id=\"T_9d7f0_row8_col2\" class=\"data row8 col2\" >0.230000</td>\n",
       "      <td id=\"T_9d7f0_row8_col3\" class=\"data row8 col3\" >0.130000</td>\n",
       "      <td id=\"T_9d7f0_row8_col4\" class=\"data row8 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d7f0_level0_row9\" class=\"row_heading level0 row9\" >Doc9</th>\n",
       "      <td id=\"T_9d7f0_row9_col0\" class=\"data row9 col0\" >0.370000</td>\n",
       "      <td id=\"T_9d7f0_row9_col1\" class=\"data row9 col1\" >0.080000</td>\n",
       "      <td id=\"T_9d7f0_row9_col2\" class=\"data row9 col2\" >0.540000</td>\n",
       "      <td id=\"T_9d7f0_row9_col3\" class=\"data row9 col3\" >0.010000</td>\n",
       "      <td id=\"T_9d7f0_row9_col4\" class=\"data row9 col4\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d7f0_level0_row10\" class=\"row_heading level0 row10\" >Doc10</th>\n",
       "      <td id=\"T_9d7f0_row10_col0\" class=\"data row10 col0\" >0.150000</td>\n",
       "      <td id=\"T_9d7f0_row10_col1\" class=\"data row10 col1\" >0.000000</td>\n",
       "      <td id=\"T_9d7f0_row10_col2\" class=\"data row10 col2\" >0.110000</td>\n",
       "      <td id=\"T_9d7f0_row10_col3\" class=\"data row10 col3\" >0.740000</td>\n",
       "      <td id=\"T_9d7f0_row10_col4\" class=\"data row10 col4\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d7f0_level0_row11\" class=\"row_heading level0 row11\" >Doc11</th>\n",
       "      <td id=\"T_9d7f0_row11_col0\" class=\"data row11 col0\" >0.250000</td>\n",
       "      <td id=\"T_9d7f0_row11_col1\" class=\"data row11 col1\" >0.190000</td>\n",
       "      <td id=\"T_9d7f0_row11_col2\" class=\"data row11 col2\" >0.340000</td>\n",
       "      <td id=\"T_9d7f0_row11_col3\" class=\"data row11 col3\" >0.230000</td>\n",
       "      <td id=\"T_9d7f0_row11_col4\" class=\"data row11 col4\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d7f0_level0_row12\" class=\"row_heading level0 row12\" >Doc12</th>\n",
       "      <td id=\"T_9d7f0_row12_col0\" class=\"data row12 col0\" >0.050000</td>\n",
       "      <td id=\"T_9d7f0_row12_col1\" class=\"data row12 col1\" >0.000000</td>\n",
       "      <td id=\"T_9d7f0_row12_col2\" class=\"data row12 col2\" >0.240000</td>\n",
       "      <td id=\"T_9d7f0_row12_col3\" class=\"data row12 col3\" >0.710000</td>\n",
       "      <td id=\"T_9d7f0_row12_col4\" class=\"data row12 col4\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d7f0_level0_row13\" class=\"row_heading level0 row13\" >Doc13</th>\n",
       "      <td id=\"T_9d7f0_row13_col0\" class=\"data row13 col0\" >0.350000</td>\n",
       "      <td id=\"T_9d7f0_row13_col1\" class=\"data row13 col1\" >0.260000</td>\n",
       "      <td id=\"T_9d7f0_row13_col2\" class=\"data row13 col2\" >0.310000</td>\n",
       "      <td id=\"T_9d7f0_row13_col3\" class=\"data row13 col3\" >0.080000</td>\n",
       "      <td id=\"T_9d7f0_row13_col4\" class=\"data row13 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d7f0_level0_row14\" class=\"row_heading level0 row14\" >Doc14</th>\n",
       "      <td id=\"T_9d7f0_row14_col0\" class=\"data row14 col0\" >0.710000</td>\n",
       "      <td id=\"T_9d7f0_row14_col1\" class=\"data row14 col1\" >0.040000</td>\n",
       "      <td id=\"T_9d7f0_row14_col2\" class=\"data row14 col2\" >0.250000</td>\n",
       "      <td id=\"T_9d7f0_row14_col3\" class=\"data row14 col3\" >0.000000</td>\n",
       "      <td id=\"T_9d7f0_row14_col4\" class=\"data row14 col4\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1d0d2236d00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Document - Topic Matrix\n",
    "lda_output = lda_model.transform(data_vectorized)\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(lda_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ede77be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3742, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_document_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9e44000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviews</th>\n",
       "      <th>verified?</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>✅ Trip Verified |  I have come to boarding and...</td>\n",
       "      <td>Trip Verified</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>✅ Trip Verified | Stinking nappies being chang...</td>\n",
       "      <td>Trip Verified</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>✅ Trip Verified | Worst service ever. Lost bag...</td>\n",
       "      <td>Trip Verified</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>✅ Trip Verified |  BA 246 21JAN 2023 Did not a...</td>\n",
       "      <td>Trip Verified</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>✅ Trip Verified | Not a great experience. I co...</td>\n",
       "      <td>Trip Verified</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            reviews  \\\n",
       "0           0  ✅ Trip Verified |  I have come to boarding and...   \n",
       "1           1  ✅ Trip Verified | Stinking nappies being chang...   \n",
       "2           2  ✅ Trip Verified | Worst service ever. Lost bag...   \n",
       "3           3  ✅ Trip Verified |  BA 246 21JAN 2023 Did not a...   \n",
       "4           4  ✅ Trip Verified | Not a great experience. I co...   \n",
       "\n",
       "       verified?  topic  \n",
       "0  Trip Verified      2  \n",
       "1  Trip Verified      1  \n",
       "2  Trip Verified      0  \n",
       "3  Trip Verified      2  \n",
       "4  Trip Verified      2  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_ = df_document_topic['dominant_topic'].tolist()\n",
    "data['topic'] = topics_\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a352d52a",
   "metadata": {},
   "source": [
    "The code you've provided is related to sentiment analysis using the VADER (Valence Aware Dictionary and sEntiment Reasoner) tool, which is a lexicon and rule-based sentiment analysis tool specifically designed for analyzing sentiments in text. Here's a breakdown of the code:\n",
    "\n",
    "1. `SentimentIntensityAnalyzer` is a class from the `nltk.sentiment.vader` module. It represents the VADER sentiment analysis tool. In the first line of the code, you're instantiating a new `SentimentIntensityAnalyzer` object and assigning it to the variable `sid`.\n",
    "\n",
    "2. Once the `SentimentIntensityAnalyzer` object is created, the `apply` method is used on the 'reviews' column of the `data` DataFrame. This method applies the `polarity_scores` function from the `sid` object to each review in the 'reviews' column. The `polarity_scores` function returns a dictionary containing the sentiment scores for each review, including the positive, negative, neutral, and compound scores.\n",
    "\n",
    "After running this code, the `sentiment_scores` variable will contain a series of dictionaries, with each dictionary representing the sentiment scores for a specific review in the 'reviews' column of the `data` DataFrame. These scores can then be used to analyze the overall sentiment of the reviews or to perform further sentiment analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b8b9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Instantiate new SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Generate sentiment scores\n",
    "sentiment_scores = data['reviews'].apply(sid.polarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74cbf06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 1, 'negative': -1}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c54edf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = {\"positive\": 1, \"negative\": -1}\n",
    "\n",
    "# Assuming your data is in a DataFrame named 'data' with a column named 'sentiment'\n",
    "data['sentiment_score'] = data['reviews'].apply(lambda x: sentiment_scores.get(x))\n",
    "\n",
    "# Now your DataFrame has a new column 'sentiment_score' with numerical values indicating sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5412e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Assuming your data is in a DataFrame named 'data' with a column named 'sentiment'\n",
    "data['sentiment_score'] = data['sentiment'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Now your DataFrame has a new column 'sentiment_score' with numerical values indicating sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dda4aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'had', 'a', 'fantastic', 'movie', 'experience!', 'the', 'acting', 'was', 'superb', 'and', 'the', 'plot', 'kept', 'me', 'on', 'the', 'edge', 'of', 'my', 'seat']\n"
     ]
    }
   ],
   "source": [
    "text = \"I had a fantastic movie experience! The acting was superb, and the plot kept me on the edge of my seat.\"\n",
    "\n",
    "cleaned_text = text.lower().replace(\",\", \"\").replace(\".\", \"\").split()\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41240c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "features = vectorizer.fit_transform(cleaned_text)\n",
    "print(features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ce6c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "features = vectorizer.fit_transform(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1d37a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<21x17 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b5b7453",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SAHEED~1\\AppData\\Local\\Temp/ipykernel_19076/2144372616.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m text = ''' The movie was great. The movie was bad. The movie was really bad \n\u001b[0;32m      6\u001b[0m '''\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtextblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtextblob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "\n",
    "text = ''' The movie was great. The movie was bad. The movie was really bad \n",
    "'''\n",
    "textblob = TextBlob()\n",
    "blob = textblob(text)\n",
    "\n",
    "for sentence in blob.sentences:\n",
    "    print(sentence.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c06ad3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "     -------------------------------------- 636.8/636.8 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\saheediyanda\\appdata\\roaming\\python\\python39\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc30698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
