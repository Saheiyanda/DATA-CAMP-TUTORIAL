{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94bd99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a55768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create and fit the model\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test features, print the results\n",
    "pred = knn.predict(X_test[0])\n",
    "print(\"Prediction for test example 0:\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440b936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saheediyanda\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9533333333333334\n",
      "0.9962880475129918\n",
      "0.9911111111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saheediyanda\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
    "\n",
    "# Apply logistic regression and print scores\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "print(lr.score(X_train,y_train))\n",
    "print(lr.score(X_test,y_test))\n",
    "\n",
    "# Apply SVM and print scores\n",
    "svm = SVC()\n",
    "svm.fit(X_train,y_train)\n",
    "print(svm.score(X_train,y_train))\n",
    "print(svm.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88210eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate logistic regression and train\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "\n",
    "# Predict sentiment for a glowing review\n",
    "review1 = \"LOVED IT! This movie was amazing. Top 10 this year.\"\n",
    "review1_features = get_features(review1)\n",
    "print(\"Review:\", review1)\n",
    "print(\"Probability of positive review:\", lr.predict_proba(review1_features)[0,1])\n",
    "\n",
    "# Predict sentiment for a poor review\n",
    "review2 = \"Total junk! I'll never watch a film by that director again, no matter how good the reviews.\"\n",
    "review2_features = get_features(review2)\n",
    "print(\"Review:\", review2)\n",
    "print(\"Probability of positive review:\", lr.predict_proba(review2_features)[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59e25569",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = datasets.load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be89086e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets._base.load_wine(*, return_X_y=False, as_frame=False)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfd8038",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w = pd.read_csv('wine_types-preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "091d5f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w.head()\n",
    "X = df_w.drop('Type',axis=1)\n",
    "y = df_w['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1562a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [LogisticRegression(),SVC(),LinearSVC(),KNeighborsClassifier()]\n",
    "\n",
    "# Fit the classifiers\n",
    "for c in classifiers:\n",
    "    c.fit(X,y)\n",
    "\n",
    "# Plot the classifiers\n",
    "plot_4_classifiers(X, y, classifiers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9803d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember that coef_ controls the angle of the boundary and intercept_ shifts the boundary without changing the angle.\n",
    "# Set the coefficients\n",
    "model.coef_ = np.array([[-1,1]])\n",
    "model.intercept_ = np.array([0])\n",
    "\n",
    "# Plot the data and decision boundary\n",
    "plot_classifier(X,y,model)\n",
    "\n",
    "# Print the number of errors\n",
    "num_err = np.sum(y != model.predict(X))\n",
    "print(\"Number of errors:\", num_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026fdaa9",
   "metadata": {},
   "source": [
    "Minimizing a loss function\n",
    "In this exercise you'll implement linear regression \"from scratch\" using scipy.optimize.minimize.\n",
    "\n",
    "We'll train a model on the Boston housing price data set, which is already loaded into the variables X and y. For simplicity, we won't include an intercept in our regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "884f4828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets._base.load_boston(*, return_X_y=False)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Boston = datasets.load_boston\n",
    "Boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f9f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The squared error, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        # Get the true and predicted target values for example 'i'\n",
    "        y_i_true = y[i]\n",
    "        y_i_pred = w@X[i]\n",
    "        s = s + (y_i_true- y_i_pred)**2\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "w_fit = minimize(my_loss, X[0]).x\n",
    "print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LinearRegression coefficients\n",
    "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4569cdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa25fe4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c79893d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 5])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.arange(3,6)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "adc8d989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x@y # this is to multiply x and y and later sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d16277f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.linspace(-2,2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e79f240e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.        , -1.55555556, -1.11111111, -0.66666667, -0.22222222,\n",
       "        0.22222222,  0.66666667,  1.11111111,  1.55555556,  2.        ])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a3df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical functions for logistic and hinge losses\n",
    "def log_loss(raw_model_output):\n",
    "   return np.log(1+np.exp(-raw_model_output))\n",
    "def hinge_loss(raw_model_output):\n",
    "   return np.maximum(0,1-raw_model_output)\n",
    "\n",
    "# Create a grid of values and plot\n",
    "grid = np.linspace(-2,2,1000)\n",
    "plt.plot(grid, log_loss(grid), label='logistic')\n",
    "plt.plot(grid, hinge_loss(grid), label='hinge')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695531f4",
   "metadata": {},
   "source": [
    "In summary, log loss in logistic regression is a measure of the accuracy of the predicted probabilities, taking into account both the correctness and confidence of the predictions. It is commonly used as the evaluation metric for logistic regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab90ebe",
   "metadata": {},
   "source": [
    "The formula for log loss is:\n",
    "\n",
    "Log Loss = -1/N * Σ(yi * log(pi) + (1 - yi) * log(1 - pi))\n",
    "\n",
    "Where:\n",
    "\n",
    "N is the number of instances\n",
    "yi is the actual outcome (0 or 1)\n",
    "pi is the predicted probability of the instance belonging to class 1\n",
    "A lower log loss value indicates better performance, with 0 being the best possible score. A higher log loss value indicates worse performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de4cf1",
   "metadata": {},
   "source": [
    "Q1. Why do we use log loss?\n",
    "A. Log loss is commonly used as an evaluation metric for binary classification tasks for several reasons. Firstly, it provides a continuous and differentiable measure of the model’s performance, making it suitable for optimization algorithms. Secondly, log loss penalizes confident and incorrect predictions more heavily, incentivizing calibrated probability estimates. Finally, log loss can be interpreted as the logarithmic measure of the likelihood of the predicted probabilities aligning with the true labels.\n",
    "\n",
    "Q2. What is a good log loss?\n",
    "A. The interpretation of a “good” log loss value depends on the specific context and problem domain. In general, a lower log loss indicates better model performance. However, what constitutes a good log loss can vary depending on the complexity of the problem, the availability of data, and the desired level of accuracy. It is often useful to compare the log loss of different models or benchmarks to assess their relative performance.\n",
    "\n",
    "Related\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6357f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logistic loss, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        raw_model_output = w@X[i]\n",
    "        s = s + log_loss(raw_model_output * y[i])\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "w_fit = minimize(my_loss, X[0]).x\n",
    "print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LogisticRegression\n",
    "lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6ccd2b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validaton errors initialized as empty list\n",
    "train_errs = list()\n",
    "valid_errs = list()\n",
    "\n",
    "# Loop over values of C_value\n",
    "for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "    # Create LogisticRegression object and fit\n",
    "    lr = LogisticRegression(C = C_value)\n",
    "    lr.fit(X_train,y_train)\n",
    "    \n",
    "    # Evaluate error rates and append to lists\n",
    "    train_errs.append( 1.0 - lr.score(X_train,y_train) )\n",
    "    valid_errs.append( 1.0 - lr.score(X_valid,y_valid) )\n",
    "    \n",
    "# Plot results\n",
    "plt.semilogx(C_values, train_errs, C_values, valid_errs)\n",
    "plt.legend((\"train\", \"validation\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1840750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify L1 regularization\n",
    "lr = LogisticRegression(penalty ='l1', solver='liblinear')\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "\n",
    "# Find the number of nonzero coefficients (selected features)\n",
    "best_lr = searcher.best_estimator_\n",
    "coefs = best_lr.coef_\n",
    "print(\"Total number of features:\", coefs.size)\n",
    "print(\"Number of selected features:\", np.count_nonzero(coefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25332b3d",
   "metadata": {},
   "source": [
    "np.argsort() is a NumPy function that returns the indices that would sort an array in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef730fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the sorted cofficients\n",
    "inds_ascending = np.argsort(lr.coef_.flatten()) \n",
    "inds_descending = inds_ascending[::-1]\n",
    "\n",
    "# Print the most positive words\n",
    "print(\"Most positive words: \", end=\"\")\n",
    "for i in range(5):\n",
    "    print(vocab[inds_descending[i]], end=\", \")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print most negative words\n",
    "print(\"Most negative words: \", end=\"\")\n",
    "for i in range(5):\n",
    "    print(vocab[inds_ascending[i]], end=\", \")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d7c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the regularization strength\n",
    "model = LogisticRegression(C=1)\n",
    "\n",
    "# Fit and plot\n",
    "model.fit(X,y)\n",
    "plot_classifier(X,y,model,proba=True)\n",
    "\n",
    "# Predict probabilities on training points\n",
    "prob = model.predict_proba(X)\n",
    "print(\"Maximum predicted probability\", np.max(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b88e64",
   "metadata": {},
   "source": [
    "In summary, the plot_classifier function is a utility function that can be used to visualize the decision boundary of a classification model. The function takes in the feature values, target values, and model object as arguments, and can plot the decision boundary based on either predicted probabilities or predicted classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19789b0",
   "metadata": {},
   "source": [
    "You got it! As you probably noticed, smaller values of C lead to less confident predictions. That's because smaller C means more regularization, which in turn means smaller coefficients, which means raw model outputs closer to zero and, thus, probabilities closer to 0.5 after the raw model output is squashed through the sigmoid function. That's quite a chain of events!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e42094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the regularization strength\n",
    "model = LogisticRegression(C=0.1)\n",
    "\n",
    "# Fit and plot\n",
    "model.fit(X,y)\n",
    "plot_classifier(X,y,model,proba=True)\n",
    "\n",
    "# Predict probabilities on training points\n",
    "prob = model.predict_proba(X)\n",
    "print(\"Maximum predicted probability\", np.max(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c49724",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "\n",
    "# Get predicted probabilities\n",
    "proba = lr.predict_proba(X)\n",
    "\n",
    "# Sort the example indices by their maximum probability\n",
    "proba_inds = np.argsort(np.max(proba,axis=1))\n",
    "\n",
    "# Show the most confident (least ambiguous) digit\n",
    "show_digit(proba_inds[-1], lr)\n",
    "\n",
    "# Show the least confident (most ambiguous) digit\n",
    "show_digit(proba_inds[0], lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c7075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit one-vs-rest logistic regression classifier\n",
    "lr_ovr = LogisticRegression(multi_class='ovr')\n",
    "lr_ovr.fit(X_train, y_train)\n",
    "\n",
    "print(\"OVR training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "print(\"OVR test accuracy    :\", lr_ovr.score(X_test, y_test))\n",
    "\n",
    "# Fit softmax classifier\n",
    "lr_mn = LogisticRegression(multi_class='multinomial',solver='lbfgs')\n",
    "lr_mn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "print(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1406683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training accuracies\n",
    "print(\"Softmax     training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "print(\"One-vs-rest training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "\n",
    "# Create the binary classifier (class 1 vs. rest)\n",
    "lr_class_1 = LogisticRegression(C=100)\n",
    "lr_class_1.fit(X_train, y_train==1)\n",
    "\n",
    "# Plot the binary classifier (class 1 vs. rest)\n",
    "plot_classifier(X_train, y_train==1, lr_class_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use SVC instead of LinearSVC from now on\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create/plot the binary classifier (class 1 vs. rest)\n",
    "svm_class_1 = SVC()\n",
    "svm_class_1.fit(X_train,y_train==1)\n",
    "plot_classifier(X_train, y_train==1,svm_class_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "462d741a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_label</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class_label  alcohol  malic_acid\n",
       "0            1    14.23        1.71\n",
       "1            1    13.20        1.78\n",
       "2            1    13.16        2.36\n",
       "3            1    14.37        1.95\n",
       "4            1    13.24        2.59"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['class_label','alcohol','malic_acid']\n",
    "df = pd.read_csv('wine.csv',usecols=columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfd83f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class_label',axis=1)\n",
    "y = df['class_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1190590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Downloading mlxtend-0.23.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from mlxtend) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from mlxtend) (1.25.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from mlxtend) (1.3.4)\n",
      "Collecting scikit-learn>=1.0.2 (from mlxtend)\n",
      "  Downloading scikit_learn-1.3.2-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from mlxtend) (3.4.3)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from mlxtend) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2021.3)\n",
      "Collecting joblib>=0.13.2 (from mlxtend)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->mlxtend) (2.2.0)\n",
      "Collecting numpy>=1.16.2 (from mlxtend)\n",
      "  Downloading numpy-1.22.4-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "     -------------------------------------- 14.7/14.7 MB 307.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: six in c:\\users\\saheediyanda\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
      "Downloading mlxtend-0.23.0-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 1.4/1.4 MB 147.4 kB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.3.2-cp39-cp39-win_amd64.whl (9.3 MB)\n",
      "   ---------------------------------------- 9.3/9.3 MB 87.3 kB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   --------------------------------------- 302.2/302.2 kB 93.9 kB/s eta 0:00:00\n",
      "Installing collected packages: numpy, joblib, scikit-learn, mlxtend\n",
      "  Attempting uninstall: numpyNote: you may need to restart the kernel to use updated packages.\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\n",
      "Successfully installed joblib-1.3.2 mlxtend-0.23.0 numpy-1.22.4 scikit-learn-1.3.2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\saheediyanda\\Anaconda3\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\saheediyanda\\Anaconda3\\Lib\\site-packages\\~klearn'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.22.4 which is incompatible.\n",
      "pandas-profiling 3.5.0 requires requests<2.29,>=2.24.0, but you have requests 2.31.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd62e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train a linear SVM\n",
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X,y)\n",
    "plot_decision_regions(X, y, svm, lims=(11,15,0,6))\n",
    "\n",
    "# Make a new data set keeping only the support vectors\n",
    "print(\"Number of original examples\", len(X))\n",
    "print(\"Number of support vectors\", len(svm.support_))\n",
    "X_small = X[svm.support_]\n",
    "y_small = y[svm.support_]\n",
    "\n",
    "# Train a new SVM using only the support vectors\n",
    "svm_small = SVC(kernel=\"linear\")\n",
    "svm_small.fit(X_small,y_small)\n",
    "# plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e04f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an RBF SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "searcher.fit(X,y)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best CV params\", searcher.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62e22ee",
   "metadata": {},
   "source": [
    "You got it! Note that the best value of gamma, 0.0001, is different from the value of 0.001 that we got in the previous exercise, when we fixed C=1. Hyperparameters can affect each other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f2c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an RBF SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "searcher.fit(X_train,y_train)\n",
    "\n",
    "# Report the best parameters and the corresponding score\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "print(\"Best CV accuracy\", searcher.best_score_)\n",
    "\n",
    "# Report the test accuracy using these best parameters\n",
    "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e0c284",
   "metadata": {},
   "source": [
    "Congrats, you finished the last exercise in the course! One advantage of SGDClassifier is that it's very fast - this would have taken a lot longer with LogisticRegression or LinearSVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeaccbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
