{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252b3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('volunteer_opportunities-preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7891a96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "# checking the missing values\n",
    "df.isna().sum()\n",
    "# checking the missing values for locality\n",
    "df['locality'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2cce4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['opportunity_id', 'content_id', 'vol_requests', 'event_time', 'title',\n",
       "       'hits', 'summary', 'is_priority', 'category_id', 'category_desc',\n",
       "       'amsl', 'amsl_unit', 'org_title', 'org_content_id', 'addresses_count',\n",
       "       'locality', 'region', 'postalcode', 'primary_loc', 'display_url',\n",
       "       'recurrence_type', 'hours', 'created_date', 'last_modified_date',\n",
       "       'start_date_date', 'end_date_date', 'status', 'Latitude', 'Longitude',\n",
       "       'Community Board', 'Community Council ', 'Census Tract', 'BIN', 'BBL',\n",
       "       'NTA'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de254f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 665 entries, 0 to 664\n",
      "Data columns (total 35 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   opportunity_id      665 non-null    int64  \n",
      " 1   content_id          665 non-null    int64  \n",
      " 2   vol_requests        665 non-null    int64  \n",
      " 3   event_time          665 non-null    int64  \n",
      " 4   title               665 non-null    object \n",
      " 5   hits                665 non-null    int64  \n",
      " 6   summary             665 non-null    object \n",
      " 7   is_priority         62 non-null     object \n",
      " 8   category_id         617 non-null    float64\n",
      " 9   category_desc       617 non-null    object \n",
      " 10  amsl                0 non-null      float64\n",
      " 11  amsl_unit           0 non-null      float64\n",
      " 12  org_title           665 non-null    object \n",
      " 13  org_content_id      665 non-null    int64  \n",
      " 14  addresses_count     665 non-null    int64  \n",
      " 15  locality            595 non-null    object \n",
      " 16  region              665 non-null    object \n",
      " 17  postalcode          659 non-null    float64\n",
      " 18  primary_loc         0 non-null      float64\n",
      " 19  display_url         665 non-null    object \n",
      " 20  recurrence_type     665 non-null    object \n",
      " 21  hours               665 non-null    int64  \n",
      " 22  created_date        665 non-null    object \n",
      " 23  last_modified_date  665 non-null    object \n",
      " 24  start_date_date     665 non-null    object \n",
      " 25  end_date_date       665 non-null    object \n",
      " 26  status              665 non-null    object \n",
      " 27  Latitude            0 non-null      float64\n",
      " 28  Longitude           0 non-null      float64\n",
      " 29  Community Board     0 non-null      float64\n",
      " 30  Community Council   0 non-null      float64\n",
      " 31  Census Tract        0 non-null      float64\n",
      " 32  BIN                 0 non-null      float64\n",
      " 33  BBL                 0 non-null      float64\n",
      " 34  NTA                 0 non-null      float64\n",
      "dtypes: float64(13), int64(8), object(14)\n",
      "memory usage: 182.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f11b2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 35)\n"
     ]
    }
   ],
   "source": [
    "# Drop the Latitude and Longitude columns from volunteer\n",
    "df_col = df.drop(['Latitude','Longitude'], axis=1)\n",
    "\n",
    "# Drop rows with missing category_desc values from volunteer_cols\n",
    "df_subset = df.dropna(subset=['category_desc'])\n",
    "\n",
    "# Print out the shape of the subset\n",
    "print(df_subset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dff14063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      737\n",
       "1       22\n",
       "2       62\n",
       "3       14\n",
       "4       31\n",
       "      ... \n",
       "660    197\n",
       "661    113\n",
       "662    145\n",
       "663    330\n",
       "664    304\n",
       "Name: hits, Length: 665, dtype: int32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['hits'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa0e7e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    737\n",
      "1     22\n",
      "2     62\n",
      "3     14\n",
      "4     31\n",
      "Name: hits, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the head of the hits column\n",
    "print(df[\"hits\"].head())\n",
    "\n",
    "# Convert the hits column to type int\n",
    "df[\"hits\"] = df['hits'].astype('int')\n",
    "\n",
    "# Look at the dtypes of the dataset\n",
    "# print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0fe2809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0                            NaN\n",
       "1      Strengthening Communities\n",
       "2      Strengthening Communities\n",
       "3      Strengthening Communities\n",
       "4                    Environment\n",
       "                 ...            \n",
       "660    Helping Neighbors in Need\n",
       "661    Strengthening Communities\n",
       "662    Helping Neighbors in Need\n",
       "663    Strengthening Communities\n",
       "664    Strengthening Communities\n",
       "Name: category_desc, Length: 665, dtype: object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category_desc'].value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3343788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X = df.drop(['category_desc'],axis=1).values\n",
    "# y = df['category_desc'].values\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faf6e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a DataFrame with all columns except category_desc\n",
    "# X = volunteer.drop('category_desc', axis=1)\n",
    "\n",
    "# # Create a category_desc labels dataset\n",
    "# y = volunteer[['category_desc']]\n",
    "\n",
    "# # Use stratified sampling to split up the dataset according to the y dataset\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# # Print the category_desc counts from y_train\n",
    "# print(y_train['category_desc'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37ad5006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "opportunity_id          0\n",
       "content_id              0\n",
       "vol_requests            0\n",
       "event_time              0\n",
       "title                   0\n",
       "hits                    0\n",
       "summary                 0\n",
       "is_priority           603\n",
       "category_id            48\n",
       "category_desc          48\n",
       "amsl                  665\n",
       "amsl_unit             665\n",
       "org_title               0\n",
       "org_content_id          0\n",
       "addresses_count         0\n",
       "locality               70\n",
       "region                  0\n",
       "postalcode              6\n",
       "primary_loc           665\n",
       "display_url             0\n",
       "recurrence_type         0\n",
       "hours                   0\n",
       "created_date            0\n",
       "last_modified_date      0\n",
       "start_date_date         0\n",
       "end_date_date           0\n",
       "status                  0\n",
       "Latitude              665\n",
       "Longitude             665\n",
       "Community Board       665\n",
       "Community Council     665\n",
       "Census Tract          665\n",
       "BIN                   665\n",
       "BBL                   665\n",
       "NTA                   665\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dd9fa46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            NaN\n",
       "1      Strengthening Communities\n",
       "2      Strengthening Communities\n",
       "3      Strengthening Communities\n",
       "4                    Environment\n",
       "                 ...            \n",
       "660    Helping Neighbors in Need\n",
       "661    Strengthening Communities\n",
       "662    Helping Neighbors in Need\n",
       "663    Strengthening Communities\n",
       "664    Strengthening Communities\n",
       "Name: category_desc, Length: 665, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category_desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52a20ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4127c0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0     1    14.23        1.71  2.43               15.6        127   \n",
       "1     1    13.20        1.78  2.14               11.2        100   \n",
       "2     1    13.16        2.36  2.67               18.6        101   \n",
       "3     1    14.37        1.95  2.50               16.8        113   \n",
       "4     1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standardization is done on numeric continous variables\n",
    "df = pd.read_csv('wine_types-preprocessing.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32c704a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Type                          178 non-null    int64  \n",
      " 1   Alcohol                       178 non-null    float64\n",
      " 2   Malic acid                    178 non-null    float64\n",
      " 3   Ash                           178 non-null    float64\n",
      " 4   Alcalinity of ash             178 non-null    float64\n",
      " 5   Magnesium                     178 non-null    int64  \n",
      " 6   Total phenols                 178 non-null    float64\n",
      " 7   Flavanoids                    178 non-null    float64\n",
      " 8   Nonflavanoid phenols          178 non-null    float64\n",
      " 9   Proanthocyanins               178 non-null    float64\n",
      " 10  Color intensity               178 non-null    float64\n",
      " 11  Hue                           178 non-null    float64\n",
      " 12  OD280/OD315 of diluted wines  178 non-null    float64\n",
      " 13  Proline                       178 non-null    int64  \n",
      "dtypes: float64(11), int64(3)\n",
      "memory usage: 19.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c42d76d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SAHEED~1\\AppData\\Local\\Temp/ipykernel_22264/3415879295.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Split the dataset into training and test sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mknn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify= y, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit the knn model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfe533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking column that is most suitable for standardization in terms of variance\n",
    "df.var()\n",
    "# it is cleared that proline is the most suitable candidate for standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the variance of the Proline column\n",
    "print(np.var(df['Proline']))\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "df['Proline_log'] = np.log(df['Proline'])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print(df['Proline_log'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4055e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct! Understanding your data is a crucial first step before deciding on the most appropriate standardization technique.\n",
    "\n",
    "df[['Ash','Alcalinity of ash','Magnesium']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77259e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Subset the DataFrame you want to scale \n",
    "df_subset = df[['Ash','Alcalinity of ash','Magnesium']]\n",
    "\n",
    "# Apply the scaler to wine_subset\n",
    "df_subset_scaled = scaler.fit_transform(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f29994",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a233c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe7402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the dataset and labels into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# # Fit the k-nearest neighbors model to the training data\n",
    "# knn.fit(X_train, y_train)\n",
    "\n",
    "# # Score the model on the test data\n",
    "# print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a967d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c25d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# # Instantiate a StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Scale the training and test features\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Fit the k-nearest neighbors model to the training data\n",
    "# knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Score the model on the test data\n",
    "# print(knn.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9622b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = pd.read_json('hiking Data-Preprocessing.json')\n",
    "enc = LabelEncoder()\n",
    "df['assesible_enc'] = enc.fit_transform(df['Accessible'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45efaabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "assesible_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[['Accessible','assesible_enc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07effdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "df_v = pd.read_csv('volunteer_opportunities-preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b92475",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5930a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category_enc = pd.get_dummies(df_v['category_desc'])\n",
    "category_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical feature engineering\n",
    "# df['month']=df['date'].applyl(lambda row: row.month)\n",
    "\n",
    "dic = {'city':['A','B','C','D'],'Day 1':[24,34,36,45],'Day 2':[33,45,66,77]}\n",
    "dic_1 =pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93499d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e94cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['Day 1','Day 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd09b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_1['mean'] = dic_1[column].apply(lambda row: row[column].mean(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6252faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85521073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use .loc to create a mean column\n",
    "# running_times_5k[\"mean\"] = running_times_5k.loc[:, 'run1':'run5'].mean(axis=1)\n",
    "\n",
    "# # Take a look at the results\n",
    "# print(running_times_5k.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b421e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to datatime\n",
    "import datetime\n",
    "df_v['date_time_converted'] = pd.to_datetime(df_v['start_date_date'])\n",
    "df_v['date_time_month'] = df_v['date_time_converted'].apply(lambda row: row.month)\n",
    "df_v['date_time_year'] = df_v['date_time_converted'].apply(lambda row: row.year)\n",
    "print(df_v[['date_time_converted','date_time_month','date_time_year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac05630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5567fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h = pd.read_csv('hiking Data-Preprocessing.json')\n",
    "df_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write a pattern to extract numbers and decimals\n",
    "# def return_mileage(length):\n",
    "    \n",
    "#     # Search the text for matches\n",
    "#     mile = re.search('\\d+\\.\\d+', length)\n",
    "    \n",
    "#     # If a value is returned, use group(0) to return the found value\n",
    "#     if mile is not None:\n",
    "#         return float(mile.group(0))\n",
    "        \n",
    "# # Apply the function to the Length column and take a look at both columns\n",
    "# df_h[\"Length_num\"] = df_h[\"Length\"].apply(return_mileage)\n",
    "# print(df_h[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdda8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Td vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "df_v.head()\n",
    "title_text = df_v['title']\n",
    "# Take the title text\n",
    "title_text = df_v['title']\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e992491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc830a5",
   "metadata": {},
   "outputs": [],
   "source": [
    " vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb20c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_row = dict(zip(text_tfidf[3].indices,text_tfidf[3].data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fb8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zipped_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the rest of the arguments\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf78f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Call the return_weights function and extend filter_list\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "        \n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)\n",
    "\n",
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_,text_tfidf, 3)\n",
    "\n",
    "# Filter the columns in text_tfidf to only those in filtered_words\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ce963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset according to the class distribution of category_desc\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_text.toarray(),y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train,y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7321658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c103ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# Define the features and labels from the wine dataset\n",
    "X = wine.drop('Type', axis=1)\n",
    "y = wine[\"Type\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Apply PCA to the wine dataset X vector\n",
    "pca_X_train = pca.fit_transform(X_train)\n",
    "pca_X_test = pca.transform(X_test)\n",
    "\n",
    "# Look at the percentage of variance explained by the different components\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0cae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit knn to the training data\n",
    "knn.fit(pca_X_train,y_train)\n",
    "\n",
    "# Score knn on the test data and print it out\n",
    "knn.score(pca_X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcefbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the dataset according to the class distribution of category_desc\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# nb = GaussianNB()\n",
    "# y = df_v[\"category_desc\"]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# # Fit the model to the training data\n",
    "# nb.fit(X_train, y_train)\n",
    "\n",
    "# # Print out the model's accuracy\n",
    "# print(nb.score(X_test, y_test))\n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of redundant column names to drop\n",
    "to_drop = [\"category_desc\", \"created_date\", \"locality\", \"region\", \"vol_requests\"]\n",
    "\n",
    "# Drop those columns from the dataset\n",
    "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
    "\n",
    "# Print out the head of volunteer_subset\n",
    "print(volunteer_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d694fcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w = pd.read_csv('wine_types-preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048726d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the column correlations of the wine dataset\n",
    "print(df_w.corr())\n",
    "\n",
    "# Drop that column from the DataFrame\n",
    "wine = df_w.drop(\"Flavanoids\", axis=1)\n",
    "\n",
    "print(wine.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7070c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the DataFrame info\n",
    "print(ufo.info())\n",
    "\n",
    "# Change the type of seconds to float\n",
    "ufo[\"seconds\"] = ufo[\"seconds\"].astype('float')\n",
    "\n",
    "# Change the date column to type datetime\n",
    "ufo[\"date\"] = pd.to_datetime(ufo['date'])\n",
    "\n",
    "# Check the column types\n",
    "print(ufo.dtypes)\n",
    "\n",
    "# Count the missing values in the length_of_time, state, and type columns, in that order\n",
    "print(ufo[['length_of_time', 'state', 'type']].isna().sum())\n",
    "\n",
    "# Drop rows where length_of_time, state, or type are missing\n",
    "ufo_no_missing = ufo.dropna(subset=['length_of_time', 'state', 'type'])\n",
    "\n",
    "# Print out the shape of the new dataset\n",
    "print(ufo_no_missing.shape)\n",
    "\n",
    "def return_minutes(time_string):\n",
    "    \n",
    "    # Search for numbers in time_string\n",
    "    num = re.search(\"\\d+\", time_string)\n",
    "    if num is not None:\n",
    "        return int(num.group(0))\n",
    "        \n",
    "# Apply the extraction to the length_of_time column\n",
    "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)\n",
    "\n",
    "# Take a look at the head o\n",
    "\n",
    "# Check the variance of the seconds and minutes columns\n",
    "print(ufo[['seconds','minutes']].var())\n",
    "\n",
    "# Log normalize the seconds column\n",
    "ufo[\"seconds_log\"] = np.log(ufo['seconds'])\n",
    "\n",
    "# Print out the variance of just the seconds_log column\n",
    "print(ufo['seconds_log'].var())\n",
    "\n",
    "\n",
    "# Use pandas to encode us values as 1 and others as 0\n",
    "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda x:1 if x=='us' else 0)\n",
    "\n",
    "# Print the number of unique type values\n",
    "print(len(ufo['type'].unique()))\n",
    "\n",
    "# Create a one-hot encoded set of the type values\n",
    "type_set = pd.get_dummies(ufo['type'])\n",
    "\n",
    "# Concatenate this set back to the ufo DataFrame\n",
    "ufo = pd.concat([ufo, type_set], axis=1)\n",
    "\n",
    "# Look at the first 5 rows of the date column\n",
    "print(ufo['date'].head())\n",
    "\n",
    "# Extract the month from the date column\n",
    "ufo[\"month\"] = ufo[\"date\"].apply(lambda row:row.month)\n",
    "\n",
    "# Extract the year from the date column\n",
    "ufo[\"year\"] = ufo[\"date\"].apply(lambda row:row.year)\n",
    "\n",
    "# Take a look at the head of all three columns\n",
    "print([ufo['date'],ufo[\"month\"],ufo[\"year\"].head()])\n",
    "\n",
    "\n",
    "# Take a look at the head of the desc field\n",
    "print(\n",
    "ufo['desc'].head())\n",
    "\n",
    "# Instantiate the tfidf vectorizer object\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform desc using vec\n",
    "desc_tfidf = vec.fit_transform(ufo['desc'])\n",
    "\n",
    "# Look at the number of columns and rows\n",
    "print(desc_tfidf.shape)\n",
    "\n",
    "# Make a list of features to drop   \n",
    "to_drop = [\"city\", \"country\", \"date\", \"desc\", \"lat\", \"length_of_time\", \"long\", \"minutes\", \"recorded\", \"seconds\", \"state\"]\n",
    "\n",
    "# Drop those features\n",
    "ufo_dropped = ufo.drop(to_drop, axis=1)\n",
    "\n",
    "# Let's also filter some words out of the text vector we created\n",
    "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)\n",
    "\n",
    "# Take a look at the features in the X set of data\n",
    "print(X.columns)\n",
    "\n",
    "# Split the X and y sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,random_state=42)\n",
    "\n",
    "# Fit knn to the training sets\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# Print the score of knn on the test sets\n",
    "print(knn.score(X_test,y_test))\n",
    "\n",
    "# Use the list of filtered words we created to filter the text vector\n",
    "filtered_text = desc_tfidf[:, list(filtered_words)]\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y \n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_text.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit nb to the training sets\n",
    "nb.fit(X_train,y_train)\n",
    "\n",
    "# Print the score of nb on the test sets\n",
    "nb.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
